{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answer System Based on the COVID-19 Data (CORD 19)\n",
    "\n",
    "Dataset: COVID-19 Open Research Dataset Challenge (CORD-19) \\\n",
    "<url :https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge>\n",
    "\n",
    "\n",
    "Members of the team:\n",
    "\n",
    "- Chinmay Dharmik | a1855351\n",
    "- Harpreet Kaur Hans | a1873328\n",
    "- Priyank Dave | a1843068\n",
    "\n",
    "Group 20 \n",
    "\n",
    "***\n",
    "\n",
    "# Section 1 - Data Preparation\n",
    "\n",
    "1. Load the dataset\n",
    "2. Preprocess the dataset\n",
    "3. Detect the language of the dataset\n",
    "4. Removing irrelevant data, Duplicate data\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries and packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import nltk as nltk\n",
    "from langdetect import detect\n",
    "from langdetect import DetectorFactory\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.max_length = 15000000\n",
    "\n",
    "# Importing the stop words list from the spacy package\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Creating a list of stop words\n",
    "stop_words = list(STOP_WORDS)\n",
    "\n",
    "# Adding custom stop words to the list\n",
    "stop_words.extend([\n",
    "    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "    'al.', 'Elsevier', 'PMC', 'CZI'\n",
    "])\n",
    "\n",
    "# Converting the list to a set of lowercased words\n",
    "stop_words = set([word.lower() for word in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Functions\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define a method to preprocess the body text of an article by applying several text cleaning operations\n",
    "def preprocess(lst):\n",
    "    # Initialize an empty list to store preprocessed text\n",
    "    pp_list = []\n",
    "    \n",
    "    # Iterate over each text in the input list\n",
    "    for text in lst:\n",
    "        # Define allowed characters to keep\n",
    "        allowed_char = string.ascii_uppercase + string.ascii_lowercase + \" .\"\n",
    "        \n",
    "        # Remove email addresses using regular expression\n",
    "        text = re.sub(r'[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}', '', text)\n",
    "        \n",
    "        # Remove DOI links using regular expression\n",
    "        text = re.sub(r'https\\:\\/\\/doi\\.org[^\\s]+', '', text)\n",
    "        \n",
    "        # Remove HTTPS links using regular expression\n",
    "        text = re.sub(r'(\\()?\\s?http(s)?\\:\\/\\/[^\\)]+(\\))?', '', text)\n",
    "        \n",
    "        # Remove single characters repeated at least 3 times for spacing error (e.g. s u m m a r y)\n",
    "        text = re.sub(r'(\\w\\s+){3,}', '', text)\n",
    "        \n",
    "        # Replace tags (e.g. [3] [4] [5]) with whitespace\n",
    "        text = re.sub(r'(\\[\\d+\\]\\,?\\s?){3,}(\\.|\\,)?', '', text)\n",
    "        \n",
    "        # Replace tags (e.g. [3, 4, 5]) with whitespace\n",
    "        text = re.sub(r'\\[[\\d\\,\\s]+\\]', '', text)\n",
    "        \n",
    "        # Replace tags (e.g. (NUM1) repeated at least 3 times with whitespace\n",
    "        text = re.sub(r'(\\(\\d+\\)\\s){3,}', '', text)\n",
    "        \n",
    "        # Replace '1.3' with '1,3' (we need it for split later)\n",
    "        text = re.sub(r'(\\d+)\\.(\\d+)', '', text)\n",
    "        \n",
    "        # Remove all full stops as abbreviations (e.g. i.e. cit. and so on)\n",
    "        text = re.sub(r'\\.(\\s)?([^A-Z\\s])', '', text)\n",
    "        \n",
    "        # Keep only allowed characters\n",
    "        text = \"\".join([x for x in text if x in allowed_char])\n",
    "        \n",
    "        # Correctly space the tokens\n",
    "        text = re.sub(r' {2,}', '', text)\n",
    "        text = re.sub(r'\\.{2,}', '', text)\n",
    "        \n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove stop words\n",
    "        # text = \" \".join([x for x in text.split(\" \") if x.lower() not in STOP_WORDS]) \n",
    "        \n",
    "        # Append preprocessed text to the list\n",
    "        pp_list.append(text)\n",
    "    \n",
    "    # Return the preprocessed text as a numpy array\n",
    "    return np.array(pp_list)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def get_body_text(file_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Given a file path, returns a list of the body texts of all entries in the JSON file\n",
    "    whose length (in number of words) is greater than 5.\n",
    "    \"\"\"\n",
    "    # initialize the body text to an empty string\n",
    "    body_text = \"\"\n",
    "\n",
    "    # open the JSON file at the specified file path\n",
    "    with open(file_path.split(\"; \")[0]) as file:\n",
    "        # load the JSON content from the file\n",
    "        content = json.load(file)\n",
    "        # extract the body text of each entry whose length is greater than 5\n",
    "        body_text = [entry['text'] for entry in content['body_text'] if len(entry['text'].split(\" \")) > 5]\n",
    "\n",
    "    # return the list of body texts\n",
    "    return body_text\n",
    "\n",
    "def get_publish_date(text):\n",
    "    year_str = text[:4] # extract the first 4 characters, which should be the year\n",
    "    year_int = int(year_str) # convert the year string to an integer\n",
    "    return year_int # return the year as an integer\n",
    "\n",
    "# method to detect the language of the body_text\n",
    "def dect_lang(lst):\n",
    "    text = ' '.join(lst)\n",
    "    # Split the body text into words\n",
    "    text = text.split(\" \")\n",
    "    # Default language is English\n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        # If there are more than 50 words in the text, use the first 50 words to detect the language\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        # Otherwise, use all the words to detect the language\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # If there is an exception raised, it might be because the beginning of the document is not in a good format\n",
    "    except Exception as e:\n",
    "        # Create a set of all the words in the text\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            # Try to detect the language using all the words\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        # If there is an exception again, let's try to find any text in the abstract to label the language\n",
    "        except Exception as e:\n",
    "            lang = \"unknown\"\n",
    "            pass\n",
    "    # Return the detected language\n",
    "    return lang\n",
    "\n",
    "# method to check if the given paper is related to COVID-19 or not\n",
    "def check_covid_paper(lst):\n",
    "    text = ' '.join(lst)\n",
    "    if text.strip() == '':\n",
    "        return -1\n",
    "    # list of terms that indicate a paper is related to COVID-19\n",
    "    covid_terms = ['covid', 'coronavirus disease 19', 'sars cov 2', '2019 ncov', '2019ncov', '2019 n cov', '2019n cov',\n",
    "        'ncov 2019', 'n cov 2019', 'coronavirus 2019', 'wuhan pneumonia', 'wuhan virus', 'wuhan coronavirus',\n",
    "        'coronavirus 2', 'covid-19', 'SARS-CoV-2', '2019-nCov']\n",
    "    \n",
    "    # convert all terms to lowercase\n",
    "    covid_terms = [elem.lower() for elem in covid_terms]\n",
    "    \n",
    "    # compile the terms into a regular expression pattern for efficient matching\n",
    "    covid_terms = re.compile('|'.join(covid_terms))\n",
    "    # search for the terms in the body text of the paper (ignoring case)\n",
    "    return bool(covid_terms.search(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1056660, 19)\n",
      "Index(['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id',\n",
      "       'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
      "       'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files',\n",
      "       'url', 's2_id'],\n",
      "      dtype='object')\n",
      "(372888, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>pdf_json_files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d1aafb70c066a2068b02786f8929fd9c900897fb</td>\n",
       "      <td>Clinical features of culture-proven Mycoplasma...</td>\n",
       "      <td>10.1186/1471-2334-1-6</td>\n",
       "      <td>2001-07-04</td>\n",
       "      <td>Madani, Tariq A; Al-Ghamdi, Aisha A</td>\n",
       "      <td>document_parses/pdf_json/d1aafb70c066a2068b027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6b0567729c2143a66d737eb0a2f63f2dce2e5a7d</td>\n",
       "      <td>Nitric oxide: a pro-inflammatory mediator in l...</td>\n",
       "      <td>10.1186/rr14</td>\n",
       "      <td>2000-08-15</td>\n",
       "      <td>Vliet, Albert van der; Eiserich, Jason P; Cros...</td>\n",
       "      <td>document_parses/pdf_json/6b0567729c2143a66d737...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06ced00a5fc04215949aa72528f2eeaae1d58927</td>\n",
       "      <td>Surfactant protein-D and pulmonary host defense</td>\n",
       "      <td>10.1186/rr19</td>\n",
       "      <td>2000-08-25</td>\n",
       "      <td>Crouch, Erika C</td>\n",
       "      <td>document_parses/pdf_json/06ced00a5fc04215949aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>348055649b6b8cf2b9a376498df9bf41f7123605</td>\n",
       "      <td>Role of endothelin-1 in lung disease</td>\n",
       "      <td>10.1186/rr44</td>\n",
       "      <td>2001-02-22</td>\n",
       "      <td>Fagan, Karen A; McMurtry, Ivan F; Rodman, David M</td>\n",
       "      <td>document_parses/pdf_json/348055649b6b8cf2b9a37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5f48792a5fa08bed9f56016f4981ae2ca6031b32</td>\n",
       "      <td>Gene expression in epithelial cells in respons...</td>\n",
       "      <td>10.1186/rr61</td>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>Domachowske, Joseph B; Bonville, Cynthia A; Ro...</td>\n",
       "      <td>document_parses/pdf_json/5f48792a5fa08bed9f560...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha  \\\n",
       "0  d1aafb70c066a2068b02786f8929fd9c900897fb   \n",
       "1  6b0567729c2143a66d737eb0a2f63f2dce2e5a7d   \n",
       "2  06ced00a5fc04215949aa72528f2eeaae1d58927   \n",
       "3  348055649b6b8cf2b9a376498df9bf41f7123605   \n",
       "4  5f48792a5fa08bed9f56016f4981ae2ca6031b32   \n",
       "\n",
       "                                               title                    doi  \\\n",
       "0  Clinical features of culture-proven Mycoplasma...  10.1186/1471-2334-1-6   \n",
       "1  Nitric oxide: a pro-inflammatory mediator in l...           10.1186/rr14   \n",
       "2    Surfactant protein-D and pulmonary host defense           10.1186/rr19   \n",
       "3               Role of endothelin-1 in lung disease           10.1186/rr44   \n",
       "4  Gene expression in epithelial cells in respons...           10.1186/rr61   \n",
       "\n",
       "  publish_time                                            authors  \\\n",
       "0   2001-07-04                Madani, Tariq A; Al-Ghamdi, Aisha A   \n",
       "1   2000-08-15  Vliet, Albert van der; Eiserich, Jason P; Cros...   \n",
       "2   2000-08-25                                    Crouch, Erika C   \n",
       "3   2001-02-22  Fagan, Karen A; McMurtry, Ivan F; Rodman, David M   \n",
       "4   2001-05-11  Domachowske, Joseph B; Bonville, Cynthia A; Ro...   \n",
       "\n",
       "                                      pdf_json_files  \n",
       "0  document_parses/pdf_json/d1aafb70c066a2068b027...  \n",
       "1  document_parses/pdf_json/6b0567729c2143a66d737...  \n",
       "2  document_parses/pdf_json/06ced00a5fc04215949aa...  \n",
       "3  document_parses/pdf_json/348055649b6b8cf2b9a37...  \n",
       "4  document_parses/pdf_json/5f48792a5fa08bed9f560...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the MetaData\n",
    "\n",
    "Dataset = pd.read_csv('metadata.csv', low_memory=False)\n",
    "print(Dataset.shape)\n",
    "print(Dataset.columns)\n",
    "Dataset.drop_duplicates(subset=['cord_uid'], inplace=True)        \n",
    "Dataset = Dataset[['sha','title', 'doi', 'publish_time', 'authors','pdf_json_files']]\n",
    "Dataset = Dataset[Dataset['pdf_json_files'].notna()]                         # considering only pdf data files\n",
    "Dataset = Dataset[Dataset['pdf_json_files'].str.endswith('.json')]           # filtering only valid json files\n",
    "print(Dataset.shape)  \n",
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(15) as pool:\n",
    "    Dataset['publish_time'] = pool.map(get_publish_date, Dataset.publish_time)\n",
    "Dataset=Dataset[Dataset['publish_time']>2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>44449ad1cca160ce491d7624f8ae1028f3570c45</td>\n",
       "      <td>Dexmedetomidine improved renal function in pat...</td>\n",
       "      <td>10.1186/s40560-019-0415-z</td>\n",
       "      <td>2020</td>\n",
       "      <td>Nakashima, Tsuyoshi; Miyamoto, Kyohei; Shima, ...</td>\n",
       "      <td>document_parses/pdf_json/44449ad1cca160ce491d7...</td>\n",
       "      <td>[Dexmedetomidine is a sedative drug that has a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>def41c08c3cb1b3752bcff34d3aed7f8486e1c86</td>\n",
       "      <td>Aortic volume determines global end-diastolic ...</td>\n",
       "      <td>10.1186/s40635-019-0284-8</td>\n",
       "      <td>2020</td>\n",
       "      <td>Akohov, Aleksej; Barner, Christoph; Grimmer, S...</td>\n",
       "      <td>document_parses/pdf_json/def41c08c3cb1b3752bcf...</td>\n",
       "      <td>[Transpulmonary thermodilution is commonly use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>f5ae3f66face323615df39d838e056ab5fcc98df</td>\n",
       "      <td>Whole genome sequencing and phylogenetic analy...</td>\n",
       "      <td>10.1186/s12864-019-6400-z</td>\n",
       "      <td>2020</td>\n",
       "      <td>Kamau, Everlyn; Oketch, John W.; de Laurent, Z...</td>\n",
       "      <td>document_parses/pdf_json/f5ae3f66face323615df3...</td>\n",
       "      <td>[Human metapneumovirus (HMPV) is a single-stra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>5be75ae4e7f8c892abd8dc396b9dbd035772c84a</td>\n",
       "      <td>European intensive care physicians’ experience...</td>\n",
       "      <td>10.1186/s13756-019-0662-8</td>\n",
       "      <td>2020</td>\n",
       "      <td>Lepape, Alain; Jean, Astrid; De Waele, Jan; Fr...</td>\n",
       "      <td>document_parses/pdf_json/5be75ae4e7f8c892abd8d...</td>\n",
       "      <td>[Antimicrobial resistance (AMR) is a threat to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>1cee4a0d0e823379ec34a462a04561bf4cd736a2</td>\n",
       "      <td>Synthetic carbohydrate-based vaccines: challen...</td>\n",
       "      <td>10.1186/s12929-019-0591-0</td>\n",
       "      <td>2020</td>\n",
       "      <td>Mettu, Ravinder; Chen, Chiang-Yun; Wu, Chung-Yi</td>\n",
       "      <td>document_parses/pdf_json/1cee4a0d0e823379ec34a...</td>\n",
       "      <td>[Carbohydrate-based vaccines have a long histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>dfbc39af0a5845b6f3698657c81a4527a45a4021</td>\n",
       "      <td>Acute kidney injury in burn patients admitted ...</td>\n",
       "      <td>10.1186/s13054-019-2710-4</td>\n",
       "      <td>2020</td>\n",
       "      <td>Folkestad, Torgeir; Brurberg, Kjetil Gundro; N...</td>\n",
       "      <td>document_parses/pdf_json/dfbc39af0a5845b6f3698...</td>\n",
       "      <td>[Acute kidney injury (AKI) is a common complic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>2caf288d2b6723ea0d82657ab1ecb1199a5b3b6b</td>\n",
       "      <td>Identification of antigens presented by MHC fo...</td>\n",
       "      <td>10.1038/s41541-019-0148-y</td>\n",
       "      <td>2020</td>\n",
       "      <td>Bettencourt, Paulo; Müller, Julius; Nicastri, ...</td>\n",
       "      <td>document_parses/pdf_json/2caf288d2b6723ea0d826...</td>\n",
       "      <td>[Mycobacterium tuberculosis (M.tb), the etiolo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>81f4c4710b9844ee1b4981af6e45fe286e44caa8</td>\n",
       "      <td>Imaging of tumour response to immunotherapy</td>\n",
       "      <td>10.1186/s41747-019-0134-1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Dromain, Clarisse; Beigelman, Catherine; Pozze...</td>\n",
       "      <td>document_parses/pdf_json/81f4c4710b9844ee1b498...</td>\n",
       "      <td>[Immune checkpoint inhibitors remove inhibitor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>d6430e4ecda76d74dc295a831b9fc1914f7c04db</td>\n",
       "      <td>Quantifying the relative impact of contact het...</td>\n",
       "      <td>10.1186/s12879-019-4738-0</td>\n",
       "      <td>2020</td>\n",
       "      <td>Lei, Hao; Jones, Rachael M.; Li, Yuguo</td>\n",
       "      <td>document_parses/pdf_json/d6430e4ecda76d74dc295...</td>\n",
       "      <td>[Healthcare-associated infections (HAIs) pose ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>778903e91db14b0973295cc8e58a6cd4c7a136ba</td>\n",
       "      <td>Paediatric nurses’ general self-efficacy, perc...</td>\n",
       "      <td>10.1186/s12913-019-4878-3</td>\n",
       "      <td>2020</td>\n",
       "      <td>Cheng, Linan; Cui, Yajuan; Chen, Qian; Ye, Yan...</td>\n",
       "      <td>document_parses/pdf_json/778903e91db14b0973295...</td>\n",
       "      <td>[A contradiction between the supply and demand...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sha  \\\n",
       "4237  44449ad1cca160ce491d7624f8ae1028f3570c45   \n",
       "4238  def41c08c3cb1b3752bcff34d3aed7f8486e1c86   \n",
       "4239  f5ae3f66face323615df39d838e056ab5fcc98df   \n",
       "4240  5be75ae4e7f8c892abd8dc396b9dbd035772c84a   \n",
       "4241  1cee4a0d0e823379ec34a462a04561bf4cd736a2   \n",
       "4242  dfbc39af0a5845b6f3698657c81a4527a45a4021   \n",
       "4243  2caf288d2b6723ea0d82657ab1ecb1199a5b3b6b   \n",
       "4244  81f4c4710b9844ee1b4981af6e45fe286e44caa8   \n",
       "4245  d6430e4ecda76d74dc295a831b9fc1914f7c04db   \n",
       "4246  778903e91db14b0973295cc8e58a6cd4c7a136ba   \n",
       "\n",
       "                                                  title  \\\n",
       "4237  Dexmedetomidine improved renal function in pat...   \n",
       "4238  Aortic volume determines global end-diastolic ...   \n",
       "4239  Whole genome sequencing and phylogenetic analy...   \n",
       "4240  European intensive care physicians’ experience...   \n",
       "4241  Synthetic carbohydrate-based vaccines: challen...   \n",
       "4242  Acute kidney injury in burn patients admitted ...   \n",
       "4243  Identification of antigens presented by MHC fo...   \n",
       "4244        Imaging of tumour response to immunotherapy   \n",
       "4245  Quantifying the relative impact of contact het...   \n",
       "4246  Paediatric nurses’ general self-efficacy, perc...   \n",
       "\n",
       "                            doi  publish_time  \\\n",
       "4237  10.1186/s40560-019-0415-z          2020   \n",
       "4238  10.1186/s40635-019-0284-8          2020   \n",
       "4239  10.1186/s12864-019-6400-z          2020   \n",
       "4240  10.1186/s13756-019-0662-8          2020   \n",
       "4241  10.1186/s12929-019-0591-0          2020   \n",
       "4242  10.1186/s13054-019-2710-4          2020   \n",
       "4243  10.1038/s41541-019-0148-y          2020   \n",
       "4244  10.1186/s41747-019-0134-1          2020   \n",
       "4245  10.1186/s12879-019-4738-0          2020   \n",
       "4246  10.1186/s12913-019-4878-3          2020   \n",
       "\n",
       "                                                authors  \\\n",
       "4237  Nakashima, Tsuyoshi; Miyamoto, Kyohei; Shima, ...   \n",
       "4238  Akohov, Aleksej; Barner, Christoph; Grimmer, S...   \n",
       "4239  Kamau, Everlyn; Oketch, John W.; de Laurent, Z...   \n",
       "4240  Lepape, Alain; Jean, Astrid; De Waele, Jan; Fr...   \n",
       "4241    Mettu, Ravinder; Chen, Chiang-Yun; Wu, Chung-Yi   \n",
       "4242  Folkestad, Torgeir; Brurberg, Kjetil Gundro; N...   \n",
       "4243  Bettencourt, Paulo; Müller, Julius; Nicastri, ...   \n",
       "4244  Dromain, Clarisse; Beigelman, Catherine; Pozze...   \n",
       "4245             Lei, Hao; Jones, Rachael M.; Li, Yuguo   \n",
       "4246  Cheng, Linan; Cui, Yajuan; Chen, Qian; Ye, Yan...   \n",
       "\n",
       "                                         pdf_json_files  \\\n",
       "4237  document_parses/pdf_json/44449ad1cca160ce491d7...   \n",
       "4238  document_parses/pdf_json/def41c08c3cb1b3752bcf...   \n",
       "4239  document_parses/pdf_json/f5ae3f66face323615df3...   \n",
       "4240  document_parses/pdf_json/5be75ae4e7f8c892abd8d...   \n",
       "4241  document_parses/pdf_json/1cee4a0d0e823379ec34a...   \n",
       "4242  document_parses/pdf_json/dfbc39af0a5845b6f3698...   \n",
       "4243  document_parses/pdf_json/2caf288d2b6723ea0d826...   \n",
       "4244  document_parses/pdf_json/81f4c4710b9844ee1b498...   \n",
       "4245  document_parses/pdf_json/d6430e4ecda76d74dc295...   \n",
       "4246  document_parses/pdf_json/778903e91db14b0973295...   \n",
       "\n",
       "                                              body_text  \n",
       "4237  [Dexmedetomidine is a sedative drug that has a...  \n",
       "4238  [Transpulmonary thermodilution is commonly use...  \n",
       "4239  [Human metapneumovirus (HMPV) is a single-stra...  \n",
       "4240  [Antimicrobial resistance (AMR) is a threat to...  \n",
       "4241  [Carbohydrate-based vaccines have a long histo...  \n",
       "4242  [Acute kidney injury (AKI) is a common complic...  \n",
       "4243  [Mycobacterium tuberculosis (M.tb), the etiolo...  \n",
       "4244  [Immune checkpoint inhibitors remove inhibitor...  \n",
       "4245  [Healthcare-associated infections (HAIs) pose ...  \n",
       "4246  [A contradiction between the supply and demand...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mp.Pool(15) as pool:\n",
    "    Dataset['body_text'] = pool.map(get_body_text, Dataset.pdf_json_files)\n",
    "Dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(15) as pool:\n",
    "    Dataset['check_covid']=pool.map(check_covid_paper,Dataset.body_text)\n",
    "    Dataset['Language']=pool.map(dect_lang,Dataset.body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=Dataset[Dataset['check_covid']==True]\n",
    "Dataset=Dataset[Dataset['Language']=='en']\n",
    "Dataset.drop('check_covid',axis=1,inplace=True)\n",
    "Dataset.drop('Language',axis=1,inplace=True)\n",
    "Dataset=Dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>pdf_json_files</th>\n",
       "      <th>body_text</th>\n",
       "      <th>pp_body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6a43f1603edfe8220dc7706a2c269b7aa5370044</td>\n",
       "      <td>COVID-19: A critical care perspective informed...</td>\n",
       "      <td>10.1016/j.accpm.2020.02.002</td>\n",
       "      <td>2020</td>\n",
       "      <td>Ling, Lowell; Joynt, Gavin M.; Lipman, Jeff; C...</td>\n",
       "      <td>document_parses/pdf_json/6a43f1603edfe8220dc77...</td>\n",
       "      <td>[The world is closely watching the outbreak of...</td>\n",
       "      <td>[the world is closely watching the outbreak of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8f0df7fdd64d2f7b7763c00956a499956139a60e</td>\n",
       "      <td>The response of Milan's Emergency Medical Syst...</td>\n",
       "      <td>10.1016/s0140-6736(20)30493-1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Spina, Stefano; Marrazzo, Francesco; Migliari,...</td>\n",
       "      <td>document_parses/pdf_json/8f0df7fdd64d2f7b7763c...</td>\n",
       "      <td>[zone or China. The COVID-19 Response Team ass...</td>\n",
       "      <td>[zone or china. the covid response team assess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>014ce88b18095a1eaa877ab5aae818df6ec214f6</td>\n",
       "      <td>Viral load of SARS-CoV-2 in clinical samples</td>\n",
       "      <td>10.1016/s1473-3099(20)30113-4</td>\n",
       "      <td>2020</td>\n",
       "      <td>Pan, Yang; Zhang, Daitao; Yang, Peng; Poon, Le...</td>\n",
       "      <td>document_parses/pdf_json/014ce88b18095a1eaa877...</td>\n",
       "      <td>[, days 4-7 (R²=0·93, p&lt;0·001) , and days 7-14...</td>\n",
       "      <td>[ daysr pand daysr p., fromconfirmed cases of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1b3d032fe3b4d7bc9d975c83b058c73c3c5143b7</td>\n",
       "      <td>Treatment and Outcome of a Patient With Lung C...</td>\n",
       "      <td>10.1016/j.jtho.2020.02.025</td>\n",
       "      <td>2020</td>\n",
       "      <td>Zhang, Hongyan; Xie, Conghua; Huang, Yihua</td>\n",
       "      <td>document_parses/pdf_json/1b3d032fe3b4d7bc9d975...</td>\n",
       "      <td>[Treatment and Outcome of a Patient With Lung ...</td>\n",
       "      <td>[treatment and outcome of a patient with lung ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ce817b1fa3f295c44f1ec1bb1a93784cfa1ba4e</td>\n",
       "      <td>Liver injury in COVID-19: management and chall...</td>\n",
       "      <td>10.1016/s2468-1253(20)30057-1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Zhang, Chao; Shi, Lei; Wang, Fu-Sheng</td>\n",
       "      <td>document_parses/pdf_json/0ce817b1fa3f295c44f1e...</td>\n",
       "      <td>[are acute and resolve quickly, but the diseas...</td>\n",
       "      <td>[are acute and resolve quickly but the disease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>f5a53197117badae809250b44f55f63a19ed7019</td>\n",
       "      <td>Initiation of a new infection control system f...</td>\n",
       "      <td>10.1016/s1473-3099(20)30110-9</td>\n",
       "      <td>2020</td>\n",
       "      <td>Chen, Xuejiao; Tian, Junzhang; Li, Guanming; L...</td>\n",
       "      <td>document_parses/pdf_json/f5a53197117badae80925...</td>\n",
       "      <td>[In December, 2019, a group of patients with p...</td>\n",
       "      <td>[in decembera group of patients with pneumonia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>f863c592acdbb85b20cfd781082d3ba883dbc8f7</td>\n",
       "      <td>Enteric involvement of coronaviruses: is faeca...</td>\n",
       "      <td>10.1016/s2468-1253(20)30048-0</td>\n",
       "      <td>2020</td>\n",
       "      <td>Yeo, Charleen; Kaushal, Sanghvi; Yeo, Danson</td>\n",
       "      <td>document_parses/pdf_json/f863c592acdbb85b20cfd...</td>\n",
       "      <td>[knowledge gap of prevalence of viraemic HCV i...</td>\n",
       "      <td>[knowledge gap of prevalence of viraemic hcv i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17b7eebc3090a925fb97c774c94b7f65a4fd527d</td>\n",
       "      <td>Full spectrum of COVID-19 severity still being...</td>\n",
       "      <td>10.1016/s0140-6736(20)30308-1</td>\n",
       "      <td>2020</td>\n",
       "      <td>Xu, Zhou; Li, Shu; Tian, Shen; Li, Hao; Kong, ...</td>\n",
       "      <td>document_parses/pdf_json/17b7eebc3090a925fb97c...</td>\n",
       "      <td>[the outbreak would involve testing sera of bl...</td>\n",
       "      <td>[the outbreak would involve testing sera of bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9b7cf9483474f9820bd3350faf461ddb45fc785e</td>\n",
       "      <td>Molecular characterization of SARS-CoV-2 in th...</td>\n",
       "      <td>10.1016/j.cmi.2020.03.020</td>\n",
       "      <td>2020</td>\n",
       "      <td>Bal, A.; Destras, G.; Gaymard, A.; Bouscambert...</td>\n",
       "      <td>document_parses/pdf_json/9b7cf9483474f9820bd33...</td>\n",
       "      <td>[In December 2019, a novel coronavirus emerged...</td>\n",
       "      <td>[in decembera novel coronavirus emerged in chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e7522f4e4b1359da1656fa2c5e1ee9cba9d16909</td>\n",
       "      <td>Considerations for Drug Interactions on QTc in...</td>\n",
       "      <td>10.1016/j.jacc.2020.04.016</td>\n",
       "      <td>2020</td>\n",
       "      <td>Roden, Dan M.; Harrington, Robert A.; Poppas, ...</td>\n",
       "      <td>document_parses/pdf_json/e7522f4e4b1359da1656f...</td>\n",
       "      <td>[Hydroxychloroquine and azithromycin have been...</td>\n",
       "      <td>[hydroxychloroquine and azithromycin have been...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha  \\\n",
       "0  6a43f1603edfe8220dc7706a2c269b7aa5370044   \n",
       "1  8f0df7fdd64d2f7b7763c00956a499956139a60e   \n",
       "2  014ce88b18095a1eaa877ab5aae818df6ec214f6   \n",
       "3  1b3d032fe3b4d7bc9d975c83b058c73c3c5143b7   \n",
       "4  0ce817b1fa3f295c44f1ec1bb1a93784cfa1ba4e   \n",
       "5  f5a53197117badae809250b44f55f63a19ed7019   \n",
       "6  f863c592acdbb85b20cfd781082d3ba883dbc8f7   \n",
       "7  17b7eebc3090a925fb97c774c94b7f65a4fd527d   \n",
       "8  9b7cf9483474f9820bd3350faf461ddb45fc785e   \n",
       "9  e7522f4e4b1359da1656fa2c5e1ee9cba9d16909   \n",
       "\n",
       "                                               title  \\\n",
       "0  COVID-19: A critical care perspective informed...   \n",
       "1  The response of Milan's Emergency Medical Syst...   \n",
       "2       Viral load of SARS-CoV-2 in clinical samples   \n",
       "3  Treatment and Outcome of a Patient With Lung C...   \n",
       "4  Liver injury in COVID-19: management and chall...   \n",
       "5  Initiation of a new infection control system f...   \n",
       "6  Enteric involvement of coronaviruses: is faeca...   \n",
       "7  Full spectrum of COVID-19 severity still being...   \n",
       "8  Molecular characterization of SARS-CoV-2 in th...   \n",
       "9  Considerations for Drug Interactions on QTc in...   \n",
       "\n",
       "                             doi  publish_time  \\\n",
       "0    10.1016/j.accpm.2020.02.002          2020   \n",
       "1  10.1016/s0140-6736(20)30493-1          2020   \n",
       "2  10.1016/s1473-3099(20)30113-4          2020   \n",
       "3     10.1016/j.jtho.2020.02.025          2020   \n",
       "4  10.1016/s2468-1253(20)30057-1          2020   \n",
       "5  10.1016/s1473-3099(20)30110-9          2020   \n",
       "6  10.1016/s2468-1253(20)30048-0          2020   \n",
       "7  10.1016/s0140-6736(20)30308-1          2020   \n",
       "8      10.1016/j.cmi.2020.03.020          2020   \n",
       "9     10.1016/j.jacc.2020.04.016          2020   \n",
       "\n",
       "                                             authors  \\\n",
       "0  Ling, Lowell; Joynt, Gavin M.; Lipman, Jeff; C...   \n",
       "1  Spina, Stefano; Marrazzo, Francesco; Migliari,...   \n",
       "2  Pan, Yang; Zhang, Daitao; Yang, Peng; Poon, Le...   \n",
       "3         Zhang, Hongyan; Xie, Conghua; Huang, Yihua   \n",
       "4              Zhang, Chao; Shi, Lei; Wang, Fu-Sheng   \n",
       "5  Chen, Xuejiao; Tian, Junzhang; Li, Guanming; L...   \n",
       "6       Yeo, Charleen; Kaushal, Sanghvi; Yeo, Danson   \n",
       "7  Xu, Zhou; Li, Shu; Tian, Shen; Li, Hao; Kong, ...   \n",
       "8  Bal, A.; Destras, G.; Gaymard, A.; Bouscambert...   \n",
       "9  Roden, Dan M.; Harrington, Robert A.; Poppas, ...   \n",
       "\n",
       "                                      pdf_json_files  \\\n",
       "0  document_parses/pdf_json/6a43f1603edfe8220dc77...   \n",
       "1  document_parses/pdf_json/8f0df7fdd64d2f7b7763c...   \n",
       "2  document_parses/pdf_json/014ce88b18095a1eaa877...   \n",
       "3  document_parses/pdf_json/1b3d032fe3b4d7bc9d975...   \n",
       "4  document_parses/pdf_json/0ce817b1fa3f295c44f1e...   \n",
       "5  document_parses/pdf_json/f5a53197117badae80925...   \n",
       "6  document_parses/pdf_json/f863c592acdbb85b20cfd...   \n",
       "7  document_parses/pdf_json/17b7eebc3090a925fb97c...   \n",
       "8  document_parses/pdf_json/9b7cf9483474f9820bd33...   \n",
       "9  document_parses/pdf_json/e7522f4e4b1359da1656f...   \n",
       "\n",
       "                                           body_text  \\\n",
       "0  [The world is closely watching the outbreak of...   \n",
       "1  [zone or China. The COVID-19 Response Team ass...   \n",
       "2  [, days 4-7 (R²=0·93, p<0·001) , and days 7-14...   \n",
       "3  [Treatment and Outcome of a Patient With Lung ...   \n",
       "4  [are acute and resolve quickly, but the diseas...   \n",
       "5  [In December, 2019, a group of patients with p...   \n",
       "6  [knowledge gap of prevalence of viraemic HCV i...   \n",
       "7  [the outbreak would involve testing sera of bl...   \n",
       "8  [In December 2019, a novel coronavirus emerged...   \n",
       "9  [Hydroxychloroquine and azithromycin have been...   \n",
       "\n",
       "                                        pp_body_text  \n",
       "0  [the world is closely watching the outbreak of...  \n",
       "1  [zone or china. the covid response team assess...  \n",
       "2  [ daysr pand daysr p., fromconfirmed cases of ...  \n",
       "3  [treatment and outcome of a patient with lung ...  \n",
       "4  [are acute and resolve quickly but the disease...  \n",
       "5  [in decembera group of patients with pneumonia...  \n",
       "6  [knowledge gap of prevalence of viraemic hcv i...  \n",
       "7  [the outbreak would involve testing sera of bl...  \n",
       "8  [in decembera novel coronavirus emerged in chi...  \n",
       "9  [hydroxychloroquine and azithromycin have been...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mp.Pool(15) as pool:\n",
    "    Dataset['pp_body_text'] = pool.map(preprocess, Dataset.body_text)\n",
    "Dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(Dataset, open('dataset.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "## Section 2: Relevant Paragraphs extraction\n",
    "\n",
    "1. Getting a list of paragraphs from the dataset\n",
    "2. Entity extraction and Indexing\n",
    "3. Paragraph Ranking Using Spacy Similarity\n",
    "4. Creating a list of top 10 paragraphs to be used as context for the Question Answering Model\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lib = Dataset.pp_body_text.tolist()     #   getting the preprocessed body text \n",
    "lib = list(itertools.chain(*lib))       #   flattening the nested lists into a single list\n",
    "\n",
    "\n",
    "libBT = Dataset.body_text.tolist()      #   getting the preprocessed body text\n",
    "libBT = list(itertools.chain(*libBT))   #   flattening the nested lists into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Dataset # deleting the Dataset Dataframe to free up memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp.max_length = 15000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1824793/1824793 [38:16<00:00, 794.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Defining a function to extract named entities from the paper\n",
    "def process_paper(paper):\n",
    "    doc = nlp(str(paper))  # Converting paper to string and processing it using spacy\n",
    "    entities = set([ent.text for ent in doc.ents])  # Extracting unique named entities from the paper\n",
    "    return entities\n",
    "\n",
    "# Creating a dictionary to store the indices of papers that contain each named entity\n",
    "entity_index = {}\n",
    "\n",
    "# Using multiprocessing to process papers in parallel for faster execution\n",
    "with Pool(15) as pool:  # Creating a pool of 15 worker processes\n",
    "    entities = pool.map(process_paper, lib)  # Applying the process_paper function to each paper in lib\n",
    "\n",
    "# Iterating over the extracted entities from each paper to update the entity index\n",
    "for i, paper_entities in tqdm(enumerate(entities), total=len(entities)):\n",
    "    for entity in paper_entities:\n",
    "        if entity not in entity_index:\n",
    "            entity_index[entity] = set()  # Creating a new set for the entity if it doesn't exist in the index\n",
    "        entity_index[entity].add(i)  # Adding the index of the paper that contains the entity to the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(entity_index, open('entity_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# entity_index = pickle.load(open('entity_index.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Functions for Section 2\n",
    "\n",
    "# Define a function to compute the similarity between a question and an article\n",
    "def compute_similarity_mp(args):\n",
    "    # Unpack the arguments\n",
    "    question, article = args\n",
    "    # Create Doc objects for the question and article using the nlp pipeline\n",
    "    question_doc = nlp(question)\n",
    "    article_doc = nlp(str(lib[article]))\n",
    "    # Compute the similarity between the question and article and return a tuple with the article index and the similarity score\n",
    "    return (article, question_doc.similarity(article_doc))\n",
    "\n",
    "# Define a function to search for articles that match a given query using multiprocessing\n",
    "def search_mp(query, entity_index):\n",
    "    # Create a Doc object for the query using the nlp pipeline\n",
    "    doc = nlp(query)\n",
    "    # Extract the named entities from the query and store them in a set\n",
    "    query_entities = set([ent.text for ent in doc.ents])\n",
    "    # Initialize a set of article indices that match all the named entities in the query\n",
    "    matching_articles = set(range(len(libBT)))\n",
    "    for entity in tqdm(query_entities):\n",
    "        # Check if the named entity is in the entity index\n",
    "        if entity in entity_index:\n",
    "            # Update the set of matching articles to only include articles that contain the named entity\n",
    "            matching_articles &= entity_index[entity]\n",
    "    # Create a multiprocessing pool with 15 workers\n",
    "    pool = mp.Pool(15)\n",
    "    # Create a list of arguments to pass to the compute_similarity_mp function\n",
    "    args = [(query, i) for i in matching_articles]\n",
    "    # Use the multiprocessing pool to compute the similarity scores for each matching article\n",
    "    results = pool.map(compute_similarity_mp, args)\n",
    "    # Sort the results by similarity score in descending order\n",
    "    results.sort(key=lambda x: x[-1], reverse=True)\n",
    "    # Close the multiprocessing pool and wait for all tasks to complete\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    # Return the top 10 matching articles\n",
    "    return results[:10]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Section 3\n",
    "\n",
    "[1] Prompt the user to input a question. \\\n",
    "[2] Call the search_mp() function to search for the user's query in the list of paragraphs and store the results in the result variable.\\\n",
    "[3] Check if the result variable is empty. If it is, notify the user that no results were found and exit the function.\\\n",
    "[4] Concatenate the relevant paragraphs into a single context.\\\n",
    "[5] Use the pre-trained Roberta model to find the answer to the user's query within the context.\\[6] Replace the question words in the user's query with the [CLS] token, and combine it with the answer to form a new answer_text string.\\\n",
    "[7] Use the pre-trained Bart model to generate a summary of the answer.\\\n",
    "[8] Prompt the user to choose whether or not to hear the answer.\\\n",
    "[9] If the user wants to hear the answer, call the text_to_speech() function to speak the answer aloud.\\\n",
    "[10] Print the user's question and the generated answer to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "def text_to_speech(text):\n",
    "    # Initialize the pyttsx3 engine\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    # Set the speech rate and volume\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', 183)\n",
    "    engine.setProperty('volume', 1)\n",
    "\n",
    "    # Convert the text to speech\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4784616\n"
     ]
    }
   ],
   "source": [
    "print(len(entity_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for answer for the question: When was covid-19 outbreak declared as a global pandemic? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: March 11\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 29. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When was covid-19 outbreak declared as a global pandemic?\n",
      "Answer: Was covid-19 outbreak declared as a global pandemic? [SEP] March 11.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: What health measures should be taken for covid-19? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 210.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: .\"\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 26. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What health measures should be taken for covid-19?\n",
      "Answer: \"What health measures should be taken for covid-19? [SEP] .\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: How to control the spread of coronavirus? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: total lockdown is in place in India from 24 th March 2020 for 21 days\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 38. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How to control the spread of coronavirus?\n",
      "Answer: Total lockdown is in place in India from 24 th March 2020 for 21 days. [CLS] to control the spread of coronavirus?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: Where did coronavirus start? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: Wuhan, Hubei Province, China\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 30. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where did coronavirus start?\n",
      "Answer: Did coronavirus start in Wuhan, China?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: How does covid-19 spread? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 33.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: tracing contacts within 3 days of cases being confirmed\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 30. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does covid-19 spread?\n",
      "Answer: Covid-19 can spread within 3 days of cases being confirmed, according to the CDC.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: What are the symptoms of coronavirus? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: cough, fever, and breathlessness\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 29. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the symptoms of coronavirus?\n",
      "Answer: What are the symptoms of coronavirus? cough, fever, and breathlessness.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: Which country became the epicenter of the global pandemic in 2021? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 53.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: Saudi Arabia\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 29. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which country became the epicenter of the global pandemic in 2021?\n",
      "Answer: Saudi Arabia could become the epicenter of the global pandemic in 2021.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: What could be the origin of the coronavirus? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 73.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: China\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 25. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What could be the origin of the coronavirus?\n",
      "Answer: China could be the origin of the coronavirus?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: What are the vaccines approved against covid-19? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 56.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: 2021\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 25. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the vaccines approved against covid-19?\n",
      "Answer: Vaccines approved against covid-19 will be available in 2021.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for answer for the question: What causes covid-19? in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 605.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 55\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Call the answer function to begin the question-answering process\u001b[39;00m\n\u001b[1;32m     45\u001b[0m questions \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mWhen was covid-19 outbreak declared as a global pandemic?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhat health measures should be taken for covid-19?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHow to control the spread of coronavirus?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mWhat are the vaccines approved against covid-19?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mWhat causes covid-19?\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m [ query_run(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m questions ]\n",
      "Cell \u001b[0;32mIn[49], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39m# Call the answer function to begin the question-answering process\u001b[39;00m\n\u001b[1;32m     45\u001b[0m questions \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mWhen was covid-19 outbreak declared as a global pandemic?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhat health measures should be taken for covid-19?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mHow to control the spread of coronavirus?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mWhat are the vaccines approved against covid-19?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mWhat causes covid-19?\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m [ query_run(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m questions ]\n",
      "Cell \u001b[0;32mIn[49], line 11\u001b[0m, in \u001b[0;36mquery_run\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Search for relevant paragraphs using the user's query\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSearching for answer for the question: \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m in the list of paragraphs...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m result \u001b[39m=\u001b[39m search_mp(query, entity_index)\n\u001b[1;32m     13\u001b[0m \u001b[39m# If no results are found, notify the user and exit the function\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[43], line 31\u001b[0m, in \u001b[0;36msearch_mp\u001b[0;34m(query, entity_index)\u001b[0m\n\u001b[1;32m     29\u001b[0m args \u001b[39m=\u001b[39m [(query, i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m matching_articles]\n\u001b[1;32m     30\u001b[0m \u001b[39m# Use the multiprocessing pool to compute the similarity scores for each matching article\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(compute_similarity_mp, args)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Sort the results by similarity score in descending order\u001b[39;00m\n\u001b[1;32m     33\u001b[0m results\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def query_run(text):\n",
    "    \"\"\"Answers a user's query by searching through a list of paragraphs and generating a summary using pre-trained models.\"\"\"\n",
    "    \n",
    "    # Prompt user for query\n",
    "    query = text\n",
    "    \n",
    "    # Search for relevant paragraphs using the user's query\n",
    "    print(f\"Searching for answer for the question: {query} in the list of paragraphs...\")\n",
    "    result = search_mp(query, entity_index)\n",
    "    \n",
    "    # If no results are found, notify the user and exit the function\n",
    "    if len(result) == 0:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "    \n",
    "    # Concatenate the relevant paragraphs into a single context\n",
    "    context = '. '.join([libBT[a[0]] for a in result])\n",
    "    \n",
    "    # Use the Roberta model to find the answer to the user's query within the context\n",
    "    print(\"Searching the answer in the top matching paragraphs using Roberta...\")\n",
    "    qa_model = pipeline(\"question-answering\",model='deepset/roberta-base-squad2-covid')\n",
    "    ans= qa_model(question = query, context = context)\n",
    "    print(\"Answer Snippit: \" + ans['answer'])\n",
    "\n",
    "    # Replace the question words in the user's query with the [CLS] token, and combine it with the answer\n",
    "    question = query.lower().replace('what','[CLS]').replace('when','[CLS]').replace('how','[CLS]').replace('where','[CLS]').replace('which','[CLS]').replace('who','[CLS]')\n",
    "    answer_text = f\"{question} [SEP] {ans['answer']}[SEP]\"\n",
    "    \n",
    "    # Use the Bart model to generate a summary of the answer\n",
    "    print(\"Generating answer using facebook/bart-large-cnn pretrained model...\")\n",
    "    summarizer = pipeline(\"summarization\", model='facebook/bart-large-cnn')\n",
    "    summary = summarizer(answer_text, min_length=5, max_length=40)\n",
    "    \n",
    "    # Ask the user if they want to hear the answer, and speak it if they do\n",
    "    #t2s = input(\"Do you want to hear the answer? (y/n) \")\n",
    "    #if t2s.lower() == 'y':\n",
    "        #text_to_speech(summary[0]['summary_text'])\n",
    "    \n",
    "    # Print the answer to the console\n",
    "    print(f\"Question: {query}\\nAnswer: {summary[0]['summary_text']}\\n\\n\\n\\n\")\n",
    "\n",
    "# Call the answer function to begin the question-answering process\n",
    "questions = [\"When was covid-19 outbreak declared as a global pandemic?\",\n",
    "        \"What health measures should be taken for covid-19?\",\n",
    "        \"How to control the spread of coronavirus?\",\n",
    "        \"Where did coronavirus start?\",\n",
    "       \"How does covid-19 spread?\",\n",
    "       \"What are the symptoms of coronavirus?\",\n",
    "       \"Which country became the epicenter of the global pandemic in 2021?\",\n",
    "       \"What could be the origin of the coronavirus?\",\n",
    "       \"What are the vaccines approved against covid-19?\",\n",
    "       \"What causes covid-19?\"]\n",
    "[ query_run(query) for query in questions ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keyboard was interupted because of precautionary measures (cpu temperature >98 C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for answer for the question: \"What could be the origin of the coronavirus?\" in the list of paragraphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 171.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching the answer in the top matching paragraphs using Roberta...\n",
      "Answer Snippit: Huanan Seafood Market (in Wuhan, China),\n",
      "Generating answer using facebook/bart-large-cnn pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 40, but you input_length is only 38. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \"What could be the origin of the coronavirus?\"\n",
      "Answer: Huanan Seafood Market (in Wuhan, China) could be the origin of the coronavirus.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def answer():\n",
    "    \"\"\"Answers a user's query by searching through a list of paragraphs and generating a summary using pre-trained models.\"\"\"\n",
    "    \n",
    "    # Prompt user for query\n",
    "    query = input(\"What is your question? \")\n",
    "    \n",
    "    # Search for relevant paragraphs using the user's query\n",
    "    print(f\"Searching for answer for the question: {query} in the list of paragraphs...\")\n",
    "    result = search_mp(query, entity_index)\n",
    "    \n",
    "    # If no results are found, notify the user and exit the function\n",
    "    if len(result) == 0:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "    \n",
    "    # Concatenate the relevant paragraphs into a single context\n",
    "    context = '. '.join([libBT[a[0]] for a in result])\n",
    "    \n",
    "    # Use the Roberta model to find the answer to the user's query within the context\n",
    "    print(\"Searching the answer in the top matching paragraphs using Roberta...\")\n",
    "    qa_model = pipeline(\"question-answering\",model='deepset/roberta-base-squad2-covid')\n",
    "    ans= qa_model(question = query, context = context)\n",
    "    print(\"Answer Snippit: \" + ans['answer'])\n",
    "\n",
    "    # Replace the question words in the user's query with the [CLS] token, and combine it with the answer\n",
    "    question = query.lower().replace('what','[CLS]').replace('when','[CLS]').replace('how','[CLS]').replace('where','[CLS]').replace('which','[CLS]').replace('who','[CLS]')\n",
    "    answer_text = f\"{question} [SEP] {ans['answer']}[SEP]\"\n",
    "    \n",
    "    # Use the Bart model to generate a summary of the answer\n",
    "    print(\"Generating answer using facebook/bart-large-cnn pretrained model...\")\n",
    "    summarizer = pipeline(\"summarization\", model='facebook/bart-large-cnn')\n",
    "    summary = summarizer(answer_text, min_length=5, max_length=40)\n",
    "    \n",
    "    # Ask the user if they want to hear the answer, and speak it if they do\n",
    "    t2s = input(\"Do you want to hear the answer? (y/n) \")\n",
    "    if t2s.lower() == 'y':\n",
    "        text_to_speech(summary[0]['summary_text'])\n",
    "        print(\"Playing Answer Snippet audio...\")\n",
    "    else:\n",
    "        print(\"Skipping Answer Snippet audio...\")\n",
    "    \n",
    "    # Print the answer to the console\n",
    "    print(f\"Question: {query}\\nAnswer: {summary[0]['summary_text']}\")\n",
    "\n",
    "# Call the answer function to begin the question-answering process\n",
    "answer()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "[1] Sentence Transformers, \"All-MiniLM-L6-v2\", https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2, accessed on April 19, 2023.\\\n",
    "[2] Doc2Vec Library, Gensim, https://radimrehurek.com/gensim/models/doc2vec.html, accessed on April 19, 2023.\\\n",
    "[3] SciSpacy, https://allenai.github.io/scispacy/, accessed on April 19, 2023.\\\n",
    "[4] RoBERTa model, Deepset, https://huggingface.co/deepset/roberta-base-squad2-covid, accessed on April 19, 2023.\\\n",
    "[5] Squad Style annotated Squad dataset, COVID-QA, https://github.com/deepset-ai/COVID-QA/blob/master/data/question-answering/200423_covidQA.json, accessed on April 19, 2023.\\\n",
    "[6] Facebook's bart-large-cnn model, Hugging Face, https://huggingface.co/facebook/bart-large-cnn, accessed on April 19, 2023.\\\n",
    "[7] T5-Base_GNAD model, Hugging Face, https://huggingface.co/Einmalumdiewelt/T5-Base_GNAD, accessed on April 19, 2023.\\\n",
    "[8] Spacy, https://spacy.io/models, accessed on April 19, 2023.\\\n",
    "[9] pyttsx3 , https://pypi.org/project/pyttsx3/ , accessed on April 19, 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
